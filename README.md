# Natural Language Processing Tasks

Simple tasks about Natural Language Processing

# Task
Listing the 100 most used n-grams."n" is given by user

# Task 1
2 different tokenizers and 2 different lemmatizers representing the Tokenization and Lemmatization processes.

# Task 2
Comparison of some basic features (unigram, bigram, trigram numbers, POS-Tag variety numbers, etc.) between 2 corpus

# Task 5
 Basic semantic analysis of a Turkish corpus. Accordingly, in the corpus;

- a list of all words (vocabulary) and their frequencies,
- a list of bigram and trigram tokens that have been used at least 5 times,
- the most similar two words using Latent Semantic Analysis should be determined.

# Task 6
A very simple example is about show how to do word sense disambiguation using the LESK algorithm on a Turkish corpus. A corpus with a few sentences and a dictionary with a few definitions can be prepared.

# Task 7
First, low and high frequency words (for threshold = 5) should be determined in a Turkish collection. Words with low frequency will be assumed to have spelling errors. To solve this problem, a Lexical Similarity function will be used and it will be suggested that these erroneous assumed words can change to the most lexically similar word from high-frequency words. The code to be prepared must give a two-column list: low-frequency words in the first column and the most lexically similar high-frequency words in the second column.

# Task 8
First of all, English word vectors calculated by any method (e.g. Word2Vec, GloVe or fastText) should be obtained from the internet. We must also have an English corpus. Using any Semantic Similarity method that the you choose or create, the most similar sentence in the corpus should be found for a sentence given externally.


